---
title: "Power Experiment 1"
output:
  bookdown::html_document2:
      keep_md: yes
  bookdown::word_document2: default
  bookdown::pdf_document2: default
always_allow_html: true
---



```{r, message=FALSE}
# load required packages
library("lme4")        # model specification / estimation
library("lmerTest")    # get p-values for mixed models
library("broom.mixed") # extracting data from model fits 
library("tidyverse")   # data wrangling and visualisation
library("patchwork")    # combine plots

# ensure this script returns the same results on each run
set.seed(912783)
```

# Introduction

Statistical power is the probability by which we are able to detect an effect, assuming that it actually exists. The higher you set the benchmark for statistical power, the less likely you are to miss an effect in your sample.

The aim of a power analysis is find out how big a sample you need to achieve the statistical power you want. If there is an effect, more participants will make it more likely to detect it.

To give an overview of this power analysis, we will

1.  Make assumptions about the data generating process (i.e. a model and its parameters).
2.  "Draw" (i.e. simulate) a sample with a given sample size.
3.  Run the statistical analysis we plan to perform on that sample and store the estimates of the model.
4.  Calculate the power: Repeat steps two and three many times. See for how many samples the model recovers a statistically significant estimate for the parameter we set. Statistical power is just the ratio of significant samples among all samples we look at.
5.  Calculate power for different sample sizes: Repeat step four for various sample sizes. See at which sample size we achieve sufficiently high power.

# I. Building a power simulation function

## 1. Data generating process

### Variables

- **Impressivenes**. Binary variable indicating whether a text was `impressive` or `basic`.

- **Learning**: "How much do you feel you’ve learnt about human history by reading this text?" [1 - Nothing, 2 - A bit, 3 - Some, 4 - Quite a bit, 5 - A lot]

- **Impressiveness**: “How impressive do you think the findings of the archeologists described in the text are?” [1 - Not very impressive, 2 - A bit impressive, 3 - Quite impressive, 4 - Very impressive, 5 - Extremely impressive]

- **Competence**: "Would you agree that reading this text has made you think of archeologists as more competent than you thought before?" [1 - Strongly disagree, 2 - Disagree, 3 - Neither agree nor disagree, 4 - Agree, 5 - Strongly agree]

- **Trust**: "Having read this text, would you agree that you trust the discipline of archeology more than you did before?" [1 - Strongly disagree, 2 - Disagree, 3 - Neither agree nor disagree, 4 - Agree, 5 - Strongly agree]

- **Consensus**: "To which extent do you think the findings from the short text you just read reflect a minority or a majority opinion among archeologists?" [1 - Small minority, 2 - Minority, 3 - About half, 4 - Majority, 5 - Large majority]

### Main model

Since impressiveness is manipulated within participants, we use a mixed model, including a random intercept and slope for participants.

$$
\text{Outcome} = (\beta_0 + b_{0, Subject}) + (\beta_i + b_{i, Subject}) \text{Impressiveness} + \epsilon
$$

with:

-   $b_{0, Subject} \sim N(0, \tau_0)$
-   $b_{i, Subject} \sim N(0, \tau_c)$
-   $\epsilon \sim N(0, \sigma)$

where:

-   $\beta_0$ is the average intercept, i.e. average outcome rating for the basic condition

-   $b_{0, Subject}$ is the subject-specific deviation from the average intercept

-   $\beta_i$ is the average effect of `impressiveness`, i.e. the average difference in outcomes between the `impressive` and the `basic` condition.

-   $b_{i, Subject}$ is the subject-specific deviation from the average impressiveness effect

-   $\epsilon$ is the error term that represents noise not accounted for by the random effects

-   $\sigma$ is the standard deviation of the normal distribution that of the error term.

-   $\tau_0)$ and $\tau_1)$ are the respective standard deviations of the normal distributions that model by-subject deviation from the average effects $\beta_0$ and $\beta_i$, respectively. We assume that the deviations of subjects are normally distributed around the average effects (hence a mean of `0`). Some subjects will have a positive deviation (i.e. larger values than the average); some will have a negative offset (i.e. smaller values than the average).

The modelling of $b_{0, Subject}$ and $b_{i, Subject}$ as described above was simplified. It assumed that both were generated from independent normal distributions. But in fact, we do not expect those distributions to be independent of each other. We expect a subject's intercept and slope to be correlated. For example, a subject with a relatively *small intercept* (i.e. relatively low trust in the basic condition) might also report less trust in the impressive condition (i.e has a relatively *smaller effect of impressiveness*) - case in which the distributions of both random effects are *positively* correlated.

Instead of drawing to independent univariate distributions, we model two correlated distributions. We achieve this by modeling a bivariate normal distribution.

$$
\begin{bmatrix} b_{0, \text{Subject}} \\ b_{i, \text{Subject}} \end{bmatrix} \sim \text{N}\Bigg(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \tau_0^2 & \rho \tau_0 \tau_i \\ \rho \tau_0 \tau_i & \tau_i^2 \end{bmatrix}\Bigg)
$$

where

-   $\begin{bmatrix} \tau_0^2 & \rho \tau_0 \tau_i \\ \rho \tau_0 \tau_i & \tau_i^2 \end{bmatrix}$ is the variance-covariance matrix of the two distributions

-   $\rho$ is the correlation coefficient

### Parameters

We use the following prefixes to designate model parameters and sampled values:

-   `beta_*`: fixed effect parameters
-   `subj_*`: by-subject random effect parameters
-   `SU_*`: sampled values for by-subject random effects
-   `e_*`: residual sd

We use the following suffices:

-   `*_0`: intercept
-   `*_i`: convergence

Other terms:

-   `*_rho`: correlation for subject's random effects
-   `n_*`: sample size
-   `sigma`: residual (error) sd

We set the parameters based on pilot data:

```{r, message=FALSE}
# read data
pilot <- read_csv("../pilot_4/data/cleaned.csv") 

# a function that runs the model for varying outcome
run_model <- function(data, outcome) {
  
  data <- data %>% 
    rename(outcome = {{outcome}})
  
  # calculate model (note that this is a reduced model, since we cannot fit the model we'd like to here)
  model <- lmer(outcome ~ impressiveness + (1 | id), data = data) %>%
    tidy() %>%
  mutate(parameter = case_when(term == "(Intercept)" ~ "beta_0",
                               term == "impressivenessimp" ~ "beta_i",
                               term == "sd__(Intercept)" ~ "subj_0",
                               term == "sd__Observation" ~ "sigma"),
         outcome = quo_name(enquo(outcome))
         ) %>% 
  select(parameter, estimate, outcome)
}

# a function that iterates the model calculation and stores resutls in parameter list
list_parameters <- function(outcomes, ...) {
  
  parameters <- list() 
  
  for (i in outcomes) {
    # Run the model
    model <- run_model(pilot, outcome = i)
    # Split the results and store them in the list with the corresponding outcome name
    parameters[[i]] <- split(model$estimate, model$parameter)
    # Set the names of the list elements to correspond to the outcomes
    names(parameters)[length(names(parameters))] <- i
  }
  return(parameters)
}

parameters <- list_parameters(data = pilot, outcomes <- c("trust", "competence", "impressed", "learn"))

```




To these effect parameters, we have to add a set of additional parameters. We store a complete list of parameters in a list so that we can call them for functions later. For the following examples, we'll use the parameters of trust. 

```{r}
# complete list of parameters (including some that will be introduced later)
parameters_trust <- c(
  parameters$trust,
  list(
    # parameters for size of data frame
    n_subj = 100,              # n participants
    levels_impressiveness = c("basic", "impressive"),  # levels of convergence
    n_stimuli_per_level = 1, # n stimuli per participant, 
    # additonal random effects (which are pure guesses)
    subj_rho = 0.3,
    subj_i = 0.5
  )
)

# a list of parameters for examples below
beta_0 <- parameters$trust$beta_0
beta_i <- parameters$trust$beta_i
subj_0 <- parameters$trust$subj_0
sigma <- parameters$trust$sigma
# additonal random effects (which are pure guesses)
subj_rho <- 0.3
subj_i <- 0.5
n_subj <- 100 # number of subjects we arbitrarily set for this example
```

## 2. Simulate a single sample

### Stimuli

Each participant sees one stimulus per impressiveness level, i.e. 2 in total.

```{r}
# set stimuli
levels_impressiveness <- c("basic", "impressive") 
n_stimuli_per_level  <-  1 
```

Simulate all stimuli a single participant sees.

```{r}
# create stimuli per participant data
stimuli <- data.frame(
  impressiveness = rep(levels_impressiveness, each = n_stimuli_per_level)
  )
```

### Subjects

When generating the subject data, we need to draw a pair of random effects (intercept + slope of impressiveness) for each participant. Earlier, we had defined the parameters for the bivariate normal distribution we draw from. We use the function `MASS::mvrnorm` to generate these draws. To be able to use this function, we define the variance-covariance matrix between the two by-subject random effects beforehand.

```{r}
# calculate random intercept / random slope covariance
covar <- subj_rho * subj_0 * subj_i

# put values into variance-covariance matrix
cov_mx  <- matrix(
  c(subj_0^2, covar,
    covar,   subj_i^2),
  nrow = 2, byrow = TRUE)

# generate the by-subject random effects
subject_rfx <- MASS::mvrnorm(n = n_subj,
                             mu = c(SU_0 = 0, SU_i = 0),
                             Sigma = cov_mx)

# make data 
subjects <- data.frame(
  # add a subject id
  id = seq_len(n_subj),
  # add by participant pair of random effects
  subject_rfx
) 
```

Check how simulated values compare to the parameters we set earlier.

```{r}
data.frame(
  parameter = c("subj_0", "subj_i", "subj_rho"),
  value = c(subj_0, subj_i, subj_rho),
  simulated = c(sd(subjects$SU_0), sd(subjects$SU_i), 
                cor(subjects$SU_0, subjects$SU_i)
                )
)
```

### Trials (Subjects x Stimuli)

We combine the two data frames `subjects` and `stimuli` we generated. We also draw a residual error for each trial/observation (`e_ss` for error term for each combination of subject and stimulus).

```{r}
# cross subject and item IDs
trials <- expand_grid(subjects, stimuli)  %>%
  # add an error term
  # nrow(.) is the number of trials/observations, i.e. rows in the data frame
  mutate(e_ss = rnorm(nrow(.), mean = 0, sd = sigma),
         # make a numeric variable to be able to generate outcomes
         impressiveness_num = ifelse(impressiveness == "basic", 0, 1), 
         # ensure 'basic' is the baseline level
         impressiveness = relevel(as.factor(impressiveness), ref = "basic"))
```

### Calculate accuracy values

Our data frame now contains all the information to calculate the accuracy values. Note that our`beta_` parameters are not part of the data frame, but they exist in the environment since we defined them earlier. Once we have added `accuracy` and we removed the helper variables we don't need anymore, we have the data frame that we can run our statistical analysis on.

We truncate the simulated `accuracy` values so that they remain on the scale from 0 to 100. This is to mimic our actual responses. It is conservative for the power calculation, since more extreme values are increasing effect size (thereby increasing power).

```{r}
dat_sim <- trials %>%
  mutate(outcome = beta_0 + SU_0 + (beta_i + SU_i)*impressiveness_num + e_ss,
         # truncate accuracy values so that they lie between 0 and 100 only
         outcome = case_when(outcome < 1 ~ 1, 
                             outcome > 5 ~ 5,
                             TRUE ~ outcome)
         ) %>% 
  # retain only variables of interest
  select(id, outcome, impressiveness)
```

Plot the data to check values.

```{r}
# independent condition
plot <- ggplot(dat_sim,  
       aes(x = impressiveness, y = outcome, 
           color = impressiveness
       )
) +
  # predicted means
  geom_hline(yintercept = beta_0) +
  geom_hline(yintercept = (beta_0 + beta_i)) +
  # actual data
  geom_violin(alpha = 0, show.legend = FALSE) +
  stat_summary(fun = mean, geom="crossbar", show.legend = FALSE) +
  scale_color_viridis_d() +
  ggtitle("Predicted versus simulated values") 

plot
```

### Data generating function

We put all the steps to generate a single sample into a function we call `draw_single_sample`. We add an argument `outcome` to the function that allows to specify the outcome variable we want to simulate data for.

```{r}
# set up data generating function
draw_single_sample <- function(
  # outcome variable
  outcome, # takes names as input, e.g. "accuracy"
  # parameters for size of data frame
  n_subj,              # n participants
  levels_impressiveness,  # levels of convergence
  n_stimuli_per_level, # n stimuli per participant
  # parameters for modelling outcome variable
  beta_0,   # intercept
  beta_i,   # effect of impressiveness
  subj_0,   # by-subject random intercept sd
  subj_i,   # by-subject random slope sd
  subj_rho, # correlation between intercept and slope 
  sigma     # residual (error) sd (i.e. all variation that the model cannot account for)
) {
  
  # 1. create stimuli per subject data
  stimuli <- data.frame(
    impressiveness = rep(levels_impressiveness, each = n_stimuli_per_level)
  )
  
  # 2. create subject data
  
  # calculate random intercept / random slope covariance
  covar <- subj_rho * subj_0 * subj_i
  
  # put values into variance-covariance matrix
  cov_mx  <- matrix(
    c(subj_0^2, covar,
      covar,   subj_i^2),
    nrow = 2, byrow = TRUE)
  
  # generate the by-subject random effects
  subject_rfx <- MASS::mvrnorm(n = n_subj,
                               mu = c(SU_0 = 0, SU_i = 0),
                               Sigma = cov_mx)
  
  # make data 
  subjects <- data.frame(
    # add a subject id
    id = seq_len(n_subj),
    # add by participant pair of random effects
    subject_rfx)
  
  
  # 3. create trials data (subjects x stimuli) & simulate outcome variables
  
  ## cross subject and item IDs
  trials <- expand_grid(subjects, stimuli)  %>%
    # add an error term
    # nrow(.) is the number of trials/observations, i.e. rows in the data frame
    mutate(e_ss = rnorm(nrow(.), mean = 0, sd = sigma),
           # make a numeric variable to be able to generate outcomes
           impressiveness_num = ifelse(impressiveness == "basic", 0, 1), 
           # ensure 'basic' is the baseline level
           impressiveness = relevel(as.factor(impressiveness), ref = "basic"))
  
  sample <- trials %>%
    mutate(outcome = beta_0 + SU_0 + (beta_i + SU_i)*impressiveness_num + e_ss,
           # truncate accuracy values so that they lie between 0 and 100 only
           outcome = case_when(outcome < 1 ~ 1, 
                               outcome > 5 ~ 5,
                               TRUE ~ outcome)
    ) %>% 
    # retain only variables of interest
    select(id, outcome, impressiveness)
  
  return(sample)
}

# # You can test the function using the commented code below
# # (un-comment by highlighting, then command+shit+c)
# test <- do.call(draw_single_sample, parameters_trust)
```

## 3. Analyze the simulated data

We run a linear mixed model using the `lme4` package on our simulated sample.

```{r}
mod_sim <- lmer(outcome ~ impressiveness + (1 | id), 
                dat_sim)
summary(mod_sim)
```

We use the `broom.mixed` package to get a tidy table of the results. Below, we added column "parameter" and "value" to compare the estimate from the model to the parameters used to simulate the data.

```{r}
# get a tidy table of results
model_estimates <- broom.mixed::tidy(mod_sim) %>% 
  mutate(
    parameter = c("beta_0", "beta_i", "subj_0", "sigma"),
    value = c(beta_0, beta_i, subj_0, sigma),
  ) %>%
  select(effect:term, parameter, value, estimate:p.value) %>% 
  mutate_if(is.numeric, round, 3)

model_estimates %>% knitr::kable()
```

This table provides us with several estimates. What we are ultimately interested in is whether the the `p.value` of the `impressiveness` effect is smaller than the significance threshold `alpha` we set.

The `p.value` is the probability of obtaining a test statistic at least as extreme (i.e. far away from `0`) as the one observed, given the null hypothesis is true. Even in a world where there is no true effect in the population altogether, there is still a chance that looking at a random sample of that population can (misleadingly) yield an effect, i.e. an estimate other than `0`. That chance is described by the p-value. Everything else equal, the larger the estimate of the effect of convergence, the smaller the p-value.

Our `alpha` is the threshold we set, at which we consider the p-value sufficiently small to "safely" reject the null hypothesis. The smaller we set alpha, the less risk we run in mistakenly detecting an effect. The alpha is commonly set at `0.05`.

```{r}
# set alpha
alpha <- 0.05

# check significance and store result
significance <- model_estimates %>% 
  filter(term == "impressivenessimpressive") %>% 
  select(term, p.value) %>% 
  mutate(significant = ifelse(p.value < alpha, TRUE, FALSE))

significance
```

### Model estimation function

We put the model estimation into a function.

```{r}
# set up model estimation function
estimate_model <- function(alpha = 0.05, ...) {
  
  # Alpha is the probability we accept of rejecting the null hypothesis
  # when in fact it is true. The smaller alpha, the less likely the
  # results are to have occurred by "accident" (i.e. sampling variation).
  
  # ... is a shortcut that forwards any additional arguments to draw_single_sample()
  
  # draw a sample
  sample <- draw_single_sample(...)
  
  # estimate model for the drawn sample
  model_estimates <- lmer(outcome ~ impressiveness + (1 | id), sample) %>% 
    # put into tidy output
    broom.mixed::tidy()
  
  # check significance and store the result
  significance <- model_estimates %>% 
    filter(term == "impressivenessimpressive") %>% 
    select(estimate, term, p.value) %>% 
    mutate(significant = ifelse(p.value < alpha, TRUE, FALSE))

  # return the p_value (as tibble)
  return(significance)
}

# You can test the function using the commented code below
# (un-comment by highlighting, then command+shit+c)
# test <- do.call(estimate_model, parameters_trust)
```

## 4. Calculate Power

The idea of a power simulation is to draw many samples, run our statistical model on each, and check how often we get a significant estimate for `impressiveness`. The share of samples for which we get a significant result among all samples we generate is our estimate of the statistical power - our ability to detect an effect in a sample, *given that there is one in the population*.

To calculate the statistical power, we set the number of samples we want to draw (`iterations`) and then use the functions we created earlier.

```{r}
# define how many samples to generate
iterations <- 5

# we run the estimate_model() function as often as specified by `iterations` and store the data
power_data <- purrr::map_df(1:iterations, 
                              # We use do.call here since we stored the
                              # parameters in an external list. We could
                              # also define them directly in 
                              # estimate_model().
                              ~do.call(estimate_model, parameters_trust))

# we group by estimate (or `term`) 
# the calculate the share of significant estimates
power <- power_data %>% 
  group_by(term) %>% 
  summarize(power = mean(significant))
```

### Power calculation function

We put the power calculation above into a function. We slightly modify this function compared to above - for example, since that function will take some time to run, we add a print message.

```{r}
calculate_power <- function(iterations, ...) {
  
  # create data frame with model results for generated samples
  power_data <- 1:iterations %>% 
    purrr::map_df(function(x){
      # this is essentially a for loop - do the following for each 
      # element in 1:iterations
      
      results <- estimate_model(...)
      
      # To keep track of progress
      if (x %% 50 == 0) {print(paste("iteration number ", x))}
      
      return(results)
      
    })
  
  # we group by estimate (or `term`) 
  # the calculate the share of significant estimates
  power <- power_data %>% 
    group_by(term) %>% 
    summarize(power = mean(significant))
  
  return(power)
}
# # You can test the function using the commented code below 
# # (un-comment by highlighting, then command+shit+c)
# test <- do.call(calculate_power, c(parameters_trust, list(iterations = 500)))
```

## 5. Calculate Power for different sample sizes

The aim of a power analysis is to inform the number of participants to recruit, *given a desired level of statistical power*.

We set this level at 90%.

```{r}
power_threshold <- 0.9
```

We are looking for a sample size that let's us detect a statistically significant effect in (at least) 90% of samples.

To do so, we repeat the previous power calculation for various sample sizes. Here, we pick only two and a low number of iterations since it is only for illustration.

```{r warning=FALSE, message=FALSE}
# calcluate power for different sample sizes

# make a data frame with sample sizes
power_by_sample_size <- tibble(n_subj = seq(from = 25, to = 100, by = 25))

# Remove "n_subj" parameter from our initial parameter list since we want to replace it 
# with the respective sample sizes every time.
parameters_trust[["n_subj"]] <- NULL

# do the `calculate_power()` function for each sample size and store the results
# in a new variable called `power`
power_by_sample_size <- map_df(power_by_sample_size$n_subj, 
                                      ~do.call(calculate_power, 
                                               c(parameters_trust,
                                                 # sets the n_subj value according
                                                 # to the respective value in
                                                 # `power_by_sample_size`
                                                 list(n_subj = .x, 
                                                      iterations = 100))))

```

We can check the resulting data to find the first sample size where power is at least 90%.

```{r}
power_by_sample_size
```

### Power by sample size function

We can put the above power calculation for different sample sizes into a function - which, finally is the function we were looking for.

The simulations that this function executes will take quite some time. Therefore, we do not want to run it every time we render this document. Instead we want to store the output of the power simulation in a `.csv` file, and have an integrated "stop" mechanism to prevent execution when that file already exists. To achieve this, we make `file_name` a mandatory argument. If a file with that name already exists, the function will not be executed.

```{r}
power_by_sample_size <- function(file_name, sample_sizes, iterations, ...) {
  
  # only run analysis if a file with that name does not yet exists
  if (!file.exists(paste0("data/", file_name))) {
    
    # do the `calculate_power()` function for each sample size and store the results
    # in a new variable called `power`
    power <- purrr::map_df(sample_sizes, 
                           function(n){
                             # this is essentially a for loop - 
                             # do the following for each 
                             # element data$n_subj
                             
                             # To keep track of progress
                             print(paste("tested sample size = ", n))
                             
                             # run power calculation
                             result <- calculate_power(n_subj = n, 
                                                       iterations = iterations, ...)
                             # identify respective sample size
                             result$n <- n
                             
                             return(result)
                             
                           })
    
    # add some variables with information about simulation
    data <- power %>% 
      mutate(
        # add identifier variable for number of iterations
        iterations = iterations)
    
    write_csv(data, paste0("data/", file_name))
  }
}

# # You can test the function using the commented code below
# # (un-comment by highlighting, then command+shit+c)
# file_name <- "test_power_simulation.csv" # change for new analyses / or delete file to re-use same name
# test <- do.call(power_by_sample_size,
#                 c(parameters_trust,
#                   list(file_name = file_name, sample_sizes = c(5, 10, 15, 20),
#                        iterations = 100)
#                   )
#                 )
# test <- read_csv(paste0("data/", file_name))
# test
```

# II. Running the power analyses

We will run four different power simulations, one per outcome. For each outome, we rely on the respective parameters based on estimates from the pilot data. 

We set additional parameters for the simulations.

```{r}
non_pilot_parameters <- list(
  # additonal random effects (which are pure guesses)
  subj_rho = 0.3,
  subj_i = 0.5,
  # parameters for size of data frame
  sample_sizes  =  seq(from = 20, to = 140, by = 20), # different n's to simulate power for 
  levels_impressiveness = c("basic", "impressive"),  # levels of impressiveness
  n_stimuli_per_level = 1, # n stimuli per participant
  # number of iterations (per sample size)
  iterations = 1000)
```

## Trust

```{r, message=FALSE}
# power simulation for trust
do.call(power_by_sample_size, c(parameters$trust, 
                                non_pilot_parameters,
                                list(file_name = "power_trust.csv")
                                        )
                )

power_trust <- read_csv("data/power_trust.csv")
power_trust
```

## Competence

```{r, message=FALSE}
# power simulation for competence
do.call(power_by_sample_size, c(parameters$competence, 
                                non_pilot_parameters,
                                list(file_name = "power_competence.csv")
                                        )
                )

power_competence <- read_csv("data/power_competence.csv")
power_competence
```

## Perceptions of impressiveness

```{r, message=FALSE}
# power simulation for perceptions of impressiveness
do.call(power_by_sample_size, c(parameters$impressed, 
                                non_pilot_parameters,
                                list(file_name = "power_impressed.csv")
                                        )
                )

power_impressed <- read_csv("data/power_impressed.csv")
power_impressed
```

## Learning

```{r, message=FALSE}
# power simulation for learning
do.call(power_by_sample_size, c(parameters$learn, 
                                non_pilot_parameters,
                                list(file_name = "power_learn.csv")
                                        )
                )

power_learn <- read_csv("data/power_learn.csv")
power_learn
```

## Plot results

Merge all data sets

```{r}
power <- bind_rows(power_trust %>% 
                     mutate(outcome_variable = "trust"), 
                   power_competence %>% 
                     mutate(outcome_variable = "competence"), 
                   power_impressed%>% 
                     mutate(outcome_variable = "impressed"), 
                   power_learn %>% 
                     mutate(outcome_variable = "learn")) %>% 
    mutate(term = case_when(
      term == "impressivenessimpressive" ~ "impressiveness",
      TRUE ~ term)
    )

# write out file
write_csv(power, "data/power_all_outcomes.csv")
```

Plot results

```{r}
# plot results
ggplot(power, 
       aes(x = n, y = power, color = term)) +
  geom_line(size = 1.5) + 
  geom_point() +
  # add a horizontal line at 90%, our power_threshold
  geom_hline(aes(yintercept = .9), linetype = 'dashed') + 
  # Prettify!
  theme_minimal() + 
    scale_colour_viridis_d(option = "plasma") + 
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) + 
  labs(x = 'Sample Size', y = 'Power',
       caption = paste("iterations per \nsample size =", max(power$iterations))) +
  guides(color = FALSE) +
  facet_wrap(~outcome_variable)
```

## Calculate precise sample size threshold

Trust has the least steep curve, so it determines the sample size. 

```{r}
# trust
power_trust %>% filter(power >= 0.9) %>%
  arrange(power) %>% 
  slice(1)
```

We first reach a power of at least 0.9 with a sample of n = 100. 

# III. Simulating a single sample 

For the preregistration, we will use simulated data for one sample, based on the power calculations here. 

We first set the additional parameters

```{r}
single_sample_parameters <- list(
  # additonal random effects (which are pure guesses)
  subj_rho = 0.3,
  subj_i = 0.5,
  # parameters for size of data frame
  n_subj = 100,
  levels_impressiveness = c("basic", "impressive"),  # levels of impressiveness
  n_stimuli_per_level = 1 # n stimuli per participant
  )
```

We then simulate a single sample. 
```{r}

sample_trust <- do.call(draw_single_sample, c(parameters$trust, 
                                              single_sample_parameters)) %>% 
  rename(trust = outcome)

sample_competence <- do.call(draw_single_sample, c(parameters$competence, 
                                              single_sample_parameters)) %>% 
  rename(competence = outcome)

sample_learn <- do.call(draw_single_sample, c(parameters$learn, 
                                              single_sample_parameters)) %>% 
  rename(learn = outcome)

sample_impressed <- do.call(draw_single_sample, c(parameters$impressed, 
                                              single_sample_parameters)) %>% 
  rename(impressed = outcome)

sample <- left_join(sample_trust, sample_competence, by = c("id", "impressiveness")) %>%
          left_join(sample_learn, by = c("id", "impressiveness")) %>%
          left_join(sample_impressed, by = c("id", "impressiveness")) %>% 
  # make integer values
  mutate_if(is.numeric, round, digits = 0) %>% 
  # add a column consensus
  mutate(consensus = sample(1:5, size = nrow(.), replace = TRUE))

# write csv
write_csv(sample, "data/simulated_sample.csv")
```

